# Crawler configuration and settings

name = "my-first-crawler"
# Name of your crawler. Keep as unique for your user.

entry = "./main.ts"
# Relative path to the entrypoint of your crawler.

schedule = "@manual"
# How often your crawler runs.
# Set to `@manual` to only trigger manually.
# Accepts valid crontab syntax and shorthand expressions.
# Crawlers can run at most once per minute.
# Go to https://cron-ai.vercel.app for help generating a cron expression.
# Crontab syntax examples:
#         *               *                *                   *              *
#   [minute (0-59)] [hour (0-23)] [day of month (1-31)] [month (1-12)] [weekday (0-7)]
#         *               *                *                   *              *
#   schedule = "*/10 * * * *"  # every 10 minutes
#   schedule = "45 23 * * *"   # every day at 11:45pm UTC
#   schedule = "30 12 * * 2"   # every Tuesday at 12:30pm UTC
#   schedule = "59 1 14 3 *"   # every March 14 at 1:59am UTC
# Valid shorthand examples:
#   schedule = "@hourly"       # -> 0 * * * * (every hour at the beginning of the hour)
#   schedule = "@daily"        # -> 0 0 * * * (every day at midnight)
#   schedule = "@weekly"       # -> 0 0 * * 0 (every week at midnight on Sunday)
#   schedule = "@monthly"      # -> 0 0 1 * * (every month at midnight on the first of the month)

maxPagesPerCrawl = 10000
# Maximum number of pages to request per crawler run.
# Does not include retries.

[queue]
# Queue consumer settings:

batchSize = 10
# Maximum number of requests that a worker will attempt to fetch in parallel.
# Can be any integer between 1 and 100, inclusive.

maxConcurrency = "auto"
# Maximum amount of concurrent workers assigned to relieve queue pressure.
# It can be useful to set this value to avoid an origin's rate limits.
# Otherwise, it's recommended to keep this value as `auto`.
# Can be the string `auto`, or any integer between 1 and 100, inclusive.

maxRetries = 3
# Maximum number of times that an unsucessful request will be retried.
# Retries will be re-added to the queue with exponential backoff.
# Can be any integer between 1 and 100, inclusive.

[bucket]
# S3-compatible bucket settings:

enabled = false
# Whether or not to create a bucket for your crawler.
# Use buckets to store large files like images and audio.
# If you don't plan to save media files, you can leave this disabled.

[vector]
# Vector index settings:

enabled = false
# Whether or not to create a vector index for your crawler.
# Use a vector index to store and query embeddings of text and images.
# If you don't plan to generate embeddings, you can leave this disabled.

dimensions = 768
# The number of dimensions of each embedding.
# Generate embeddings with an equal number of dimensions to save in the index.
# This value cannot be changed after the index has been created.

metric = "cosine"
# The distance metric used to determine similar vectors.
# Value is one of `cosine`, `euclidean`, or `dot-product`.
# This value cannot be changed after the index has been created.
