# Crawler configuration and settings

name = "my-first-crawler"
# Name of your crawler. Keep as unique for your user.

entry = "./main.ts"
# Relative path to the entrypoint of your crawler.

schedule = "@manual"
# How often your crawler runs.
# Set to `@manual` to only trigger manually.
# Accepts valid crontab syntax and shorthand expressions.
# Crawlers can run at most once per minute.
# Go to https://cron-ai.vercel.app for help generating a cron expression.
# Crontab syntax examples:
#         *               *                *                   *              *
#   [minute (0-59)] [hour (0-23)] [day of month (1-31)] [month (1-12)] [weekday (0-7)]
#         *               *                *                   *              *
#   schedule = "*/10 * * * *"  # every 10 minutes
#   schedule = "45 23 * * *"   # every day at 11:45pm UTC
#   schedule = "30 12 * * 2"   # every Tuesday at 12:30pm UTC
#   schedule = "59 1 14 3 *"   # every March 14 at 1:59am UTC
# Valid shorthand examples:
#   schedule = "@hourly"       # -> 0 * * * * (every hour at the beginning of the hour)
#   schedule = "@daily"        # -> 0 0 * * * (every day at midnight)
#   schedule = "@weekly"       # -> 0 0 * * 0 (every week at midnight on Sunday)
#   schedule = "@monthly"      # -> 0 0 1 * * (every month at midnight on the first of the month)

[crawl]
# Per-crawl settings:

maxMinutes = 59
# Maximum duration of the crawl in minutes.
# Can be any integer between 1 and 43199, inclusive.
# Must be at least one minute less than the scheduled interval.

maxRequests = 10000
# Maximum number of pages to request per crawler run.
# Does not include retries.

maxRequestsPerOrigin = 10000
# Maximum number of requests per origin per crawler run.
# You can use this to limit the amount of visits per website.

[links]
# Link traversal settings:

allowHosts = []
# Allowlist of hostnames, like `www.example.com`. Supersedes `links.disallowHosts`.
# Provide values to limit the crawl to an explicit set of hard-coded hostnames.

allowTLDs = []
# Allowlist of top-level domains, like `.com`. Supersedes `links.disallowTlds`.
# Provide values to exclude links with TLDs not included here.

disallowHosts = []
# Denylist of hostnames, like `www.example.com`.
# Provide values to skip adding matching links to the queue.

disallowTLDs = []
# Denylist of top-level domains, like `.com`.
# Provide values to skip adding matching links to the queue.
# See https://docs.crawlspace.dev/build/configuration for platform-enforced values.

disallowExtensions = []
# Denylist of file extensions, like `.pdf`.
# Provide values to skip adding matching links to the queue.
# See https://docs.crawlspace.dev/build/configuration for platform-enforced values.

ignoreQueryParams = false
# Whether or not to remove query parameters from URLs before adding to the queue.

[queue]
# Queue consumer settings:

batchSize = 10
# Maximum number of requests that a worker will attempt to fetch in parallel.
# Can be any integer between 1 and 100, inclusive.

maxConcurrency = "auto"
# Maximum amount of concurrent workers assigned to relieve queue pressure.
# It can be useful to set this value to avoid an origin's rate limits.
# Otherwise, it's recommended to keep this value as `auto`.
# Can be the string `auto`, or any integer between 1 and 100, inclusive.

maxRetries = 3
# Maximum number of times that an unsucessful request will be retried.
# Retries will be re-added to the queue with exponential backoff.
# Can be any integer between 0 and 100, inclusive.

[vector]
# Vector database settings:

enabled = false
# Whether or not to create a vector database for your crawler.
# Use a vector database to store and query embeddings of text and images.
# If you don't plan to generate or store embeddings, you can leave this disabled.

dimensions = 768
# The number of dimensions of each embedding.
# Generate embeddings with an equal number of dimensions to save in the vector database.
# This value cannot be changed after the vector database has been created.

metric = "cosine"
# The distance metric used to determine similar vectors.
# Value is one of `cosine`, `euclidean`, or `dot-product`.
# This value cannot be changed after the vector database has been created.

[bucket]
# S3-compatible bucket settings:

enabled = false
# Whether or not to create a bucket for your crawler.
# Use buckets to store long text like markdown, or large files like images and audio.
# If you don't plan to save long text or large files, you can leave this disabled.
